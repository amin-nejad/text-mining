{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /home/aa5118/anaconda3/envs/plural/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/aa5118/anaconda3/envs/plural/lib/python3.7/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/aa5118/anaconda3/envs/plural/lib/python3.7/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import requests\n",
    "import justext\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import json\n",
    "import wikipedia\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rake_nltk import Rake\n",
    "import os\n",
    "import subprocess\n",
    "from neo4j import GraphDatabase\n",
    "from pathlib import Path\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we create a NamedEntityRecognition class. Each object of this class will represent a given webpage and specifically hold the information regarding the named entities present within the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedEntityRecognition:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: url of the webpage as a string\n",
    "        \n",
    "        Just instantiates all the lists of named entities\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.url = url\n",
    "        self.companies = []\n",
    "        self.people = []\n",
    "        self.keywords = []\n",
    "        self.locations = []\n",
    "        self.events = []\n",
    "        self.products = []\n",
    "        self.text = None\n",
    "        self.doc = None\n",
    "        \n",
    "    def visualise(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: None\n",
    "        \n",
    "        Returns a html rendered visualisation of the named entities within the homepage of the url using displacy\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if str(self.doc) != \"\":\n",
    "            return displacy.render(self.doc, style=\"ent\", jupyter=True)\n",
    "        else:\n",
    "            return \"No entities found.\"\n",
    "    \n",
    "    def jsonify(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: None\n",
    "        \n",
    "        Returns a json version of all the named entities within the webpage\n",
    "        \"\"\"\n",
    "        \n",
    "        list_of_entities = [self.companies, self.people, self.keywords, self.locations, self.events, self.products]\n",
    "        list_of_entity_types = [\"companies\", \"people\", \"keywords\", \"locations\", \"events\", \"products\"]\n",
    "        zip_object = zip(list_of_entity_types, list_of_entities)\n",
    "        entity_dict = dict(zip_object)\n",
    "        \n",
    "        return json.dumps(entity_dict, indent = 4, ensure_ascii=False)\n",
    "    \n",
    "    def get_people_wiki_pages(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: None\n",
    "        \n",
    "        Returns a dictionary consisting of the people identified within the webpage as the key, whilst the value attempts\n",
    "        to return the url and first line of the article for the closest match to the person on wikipedia\n",
    "        \n",
    "        *** STILL NEEDS WORK, RETURNS UNEXPECTED RESULTS IF THE PERSON IS NOT ON WIKIPEDIA ***\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        wiki = []\n",
    "        \n",
    "        for person in self.people:\n",
    "            try:\n",
    "                result = wikipedia.page(person)\n",
    "                summary = wikipedia.summary(person, sentences=1)\n",
    "                wiki.append([result.url, summary])\n",
    "            except:\n",
    "                wiki.append([\"\",\"\"])\n",
    "        zip_object = zip(self.people, wiki)\n",
    "        \n",
    "        return dict(zip_object)\n",
    "\n",
    "    def __get_links(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: None\n",
    "        \n",
    "        Private function which attempts to extract all the relevant links from the original webpage that point to other\n",
    "        pages on the same website and returns the urls in a list\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        new_url = \"http://\" + self.url\n",
    "        html=requests.get(new_url)\n",
    "        links = []\n",
    "\n",
    "        for link in BeautifulSoup(html.text, parse_only=SoupStrainer('a')):\n",
    "            try:\n",
    "                new_link = (link['href'])\n",
    "\n",
    "                short_url = self.url[4:]\n",
    "                if (new_link.find(short_url) >= 0) and (new_link.count('@') == 0):\n",
    "                    idx=new_link.find(short_url) + len(short_url)\n",
    "                    end=new_link.find('/',idx+1)\n",
    "                    if end == -1:\n",
    "                        end = len(new_link)\n",
    "                    links.append(new_link[:end])\n",
    "\n",
    "                elif (new_link.count('/', 1) <= 1) and (str(new_link)[:4] != \"http\") and (new_link.count('#') == 0) and (new_link.count('@') == 0):\n",
    "\n",
    "                    if (new_link.count('/', 1) == 1) and (new_link[:-1] != \"/\"):\n",
    "                        continue\n",
    "                    links.append(self.url + new_link)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return list(set(links))\n",
    "    \n",
    "    def get_salience(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: None\n",
    "        \n",
    "        Returns a measure of the relative importance of all the named entities. It attempts to do this by simply counting the\n",
    "        frequency of all the terms both on the webpage and on all the other relevant pages to which the original page links \n",
    "        using the __get_links() method. The function then returns a dictionary in descending order of count.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        corpus=[self.text]\n",
    "        links = self.__get_links()\n",
    "\n",
    "        for link in links:\n",
    "\n",
    "            if (link[:4] != \"http\"):\n",
    "                link = \"http://\" + link\n",
    "\n",
    "            try:\n",
    "                html = requests.get(link)\n",
    "            except:\n",
    "                html=\"\"\n",
    "            \n",
    "            text = \"\"\n",
    "            if str(html) == \"<Response [200]>\":\n",
    "                \n",
    "                try:\n",
    "                    paragraphs = justext.justext(html.text.encode('utf-8'), justext.get_stoplist(\"English\"))\n",
    "                    for paragraph in paragraphs:\n",
    "                        if not paragraph.is_boilerplate:\n",
    "                            text+= paragraph.text + \"\\n\"\n",
    "                except:\n",
    "                    print(\"Could not parse text\")\n",
    "            corpus.append(text)\n",
    "\n",
    "        all_entities = self.companies + self.people + self.keywords + self.locations + self.events + self.products\n",
    "        counts = []\n",
    "        \n",
    "        for entity in all_entities:\n",
    "            counts.append(sum(entity in s for s in corpus))\n",
    "        \n",
    "        entity_dict = dict(zip(all_entities, counts))\n",
    "        \n",
    "        return {k: v for k, v in sorted(entity_dict.items(), key=lambda x: x[1],reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create an object which parses a file containing a list of urls and extracts content from the webpages, including named entities, storing them in instances of the NamedEntityRecognition class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract:\n",
    "    \n",
    "    def __init__(self, csv_or_url):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: filename containing a list of url strings (with 'url' header) or an individual url string\n",
    "        \n",
    "        Takes a list of urls or single url and initialises some variables. \n",
    "        Adds a function removing whitespace to the spacy pipeline if it has not already been added\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        my_file = Path(csv_or_url)\n",
    "        \n",
    "        if my_file.is_file():\n",
    "            sources=pd.read_csv(csv_or_url,header=None,names=[\"url\"])\n",
    "        else:\n",
    "            sources = pd.DataFrame({'url':csv_or_url}, index=[0])\n",
    "        \n",
    "        self.sources = sources\n",
    "        self.bad_urls = []\n",
    "        self.extracted = self.__extract_text()\n",
    "        self.ner_list = None\n",
    "        \n",
    "        if \"__remove_whitespace_entities\" not in (dict(nlp.pipeline).keys()):\n",
    "            nlp.add_pipe(self.__remove_whitespace_entities, after='ner')\n",
    "\n",
    "    def __extract_text(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: None\n",
    "        \n",
    "        Private function which is called on initiliatisation of the object. Loops through every url in the list and extracts\n",
    "        non-boilerplate content before saving it as list. Keeps track of urls which are either broken or timeout before\n",
    "        returning the webpage and saves it in the data member \"bad_urls\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        extracted=[]\n",
    "        for url in self.sources['url']:\n",
    "\n",
    "            url = \"http://\"+url\n",
    "            \n",
    "            try:\n",
    "                html = requests.get(url)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.bad_urls.append(url)\n",
    "                print (e)\n",
    "            \n",
    "            if str(html) == \"<Response [200]>\":\n",
    "                \n",
    "                text = \"\"\n",
    "                paragraphs = justext.justext(html.text.encode('utf-8'), justext.get_stoplist(\"English\"))\n",
    "                for paragraph in paragraphs:\n",
    "                    if not paragraph.is_boilerplate:\n",
    "                        text+= paragraph.text + \"\\n\"\n",
    "            else:\n",
    "                text=\"<ERROR>\"\n",
    "\n",
    "            extracted.append(text)\n",
    "\n",
    "        return extracted\n",
    "    \n",
    "    def __remove_whitespace_entities(self, doc):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: string representing an individual document\n",
    "        \n",
    "        A function added to the spacy pipeline due to a bug in spacy which results in certain whitespace characters being \n",
    "        recognised as entities: https://github.com/explosion/spaCy/issues/2870\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
    "        return doc\n",
    "        \n",
    "    def extract_entities(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: None\n",
    "        \n",
    "        Uses the spacy namer entity recogniser to extract named entities from the non-boilerplate content of each webpage.\n",
    "        Additionally uses the Rake algorithm to extract keywords from the same non-boilerplate content. Creates an instance of\n",
    "        the NamedEntityRecognition class for each webpage and stores the addresses to the objects in a list called \"ner_list\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ner_list=[]\n",
    "        r = Rake()\n",
    "        k = 10 #top number of key words to extract\n",
    "\n",
    "        for i, url in enumerate(self.sources['url']):\n",
    "            ner_list.append(NamedEntityRecognition(url))\n",
    "            \n",
    "            ner_list[i].text = self.extracted[i]\n",
    "            r.extract_keywords_from_text(ner_list[i].text)\n",
    "            top_k_keywords = [value for value in dict(r.get_ranked_phrases_with_scores()[:k]).values()]\n",
    "            ner_list[i].keywords = top_k_keywords\n",
    "            \n",
    "            doc = nlp(self.extracted[i])\n",
    "            obj = ner_list[i]\n",
    "            obj.doc = doc\n",
    "            \n",
    "            for entity in doc.ents:\n",
    "                if (entity.label_ == \"ORG\"):\n",
    "                    if entity.text not in  obj.companies: obj.companies.append(entity.text)\n",
    "                elif (entity.label_ == \"PERSON\"):\n",
    "                    if entity.text not in  obj.people: obj.people.append(entity.text)\n",
    "                elif (entity.label_ == \"GPE\"):\n",
    "                    if entity.text not in  obj.locations: obj.locations.append(entity.text)            \n",
    "                elif (entity.label_ == \"EVENT\"):\n",
    "                    if entity.text not in  obj.events: obj.events.append(entity.text)        \n",
    "                elif (entity.label_ == \"PRODUCT\"):\n",
    "                    if entity.text not in  obj.products: obj.products.append(entity.text)\n",
    "        \n",
    "        self.ner_list = ner_list\n",
    "        return\n",
    "        \n",
    "    def get_number_of_urls(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args: None\n",
    "        \n",
    "        Simple function to just return the number of urls in the input file\n",
    "        \n",
    "        \"\"\"\n",
    "        return len(self.sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a class for adding our extracted entities and keywords to a neo4j graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4j:\n",
    "    \n",
    "    def __init__(self, NER_object):\n",
    "        \"\"\"\n",
    "        Args: an object of type NamedEntityRecognition\n",
    "        \n",
    "        Takes an object of type NamedEntityRecognition and extracts the url and JSON from the object. \n",
    "        Also connects to a neo4j database.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.uri = \"bolt://localhost:7687\"\n",
    "        self.driver = GraphDatabase.driver(self.uri, auth=(\"neo4j\", \"4jNeo\"))\n",
    "        self.json = json.loads(NER_object.jsonify())\n",
    "        self.url = NER_object.url\n",
    "        self.salience = NER_object.get_salience()\n",
    "        \n",
    "    def __create_query(self, tx, test):\n",
    "        \"\"\"\n",
    "        Args: a neo4j driver session\n",
    "        \n",
    "        Internal function which creates and runs the query that extracts the information from the JSON and\n",
    "        inserts them as nodes into the neo4j graph.\n",
    "        \"\"\"\n",
    "        temp_id = 1\n",
    "        query = (\"CREATE (name:website {url:'\"+self.url+\"'})\")\n",
    "        for i in self.json:\n",
    "            for j in self.json[i]:\n",
    "                query += \" CREATE (node\" + str(temp_id) + \":\" + i + \" {name:'\"+j+\"', salience:\" + str(self.salience[j]) +\"})\"\n",
    "                query += \" CREATE (node\" + str(temp_id) + \")-[:EXTRACTED_FROM]->(name)\"\n",
    "                temp_id += 1\n",
    "        if test is False: tx.run(query)\n",
    "        \n",
    "        return (\"Nodes added to graph:\" + str(temp_id))\n",
    "                \n",
    "    def add_nodes(self, test=False):\n",
    "        \"\"\"\n",
    "        Args: none\n",
    "        \n",
    "        External function which calls __create_function from a neo4j driver session\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            msg = session.read_transaction(self.__create_query, test)\n",
    "            \n",
    "        return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now instantiate our class and see what our webpages have to offer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpages = Extract(\"www.clear.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages.bad_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpages.extract_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www.clear.ai'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages.ner_list[0].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">Imagine a real-time knowledge graph of the global economy</br>Clear is on a mission to build the most extensive graph of the global economy ever made to enable an independent trade data and AI platform. We provide value-added data products and commercial insights to our partners as we work towards becoming a global utility. Our goal is to drive transparency, efficiency and opportunity in markets everywhere, for the benefit of anyone who does business.</br>A new perspective on global trade</br>As goods and services make their journey around the world, from supplier to buyer to customer, they leave behind disjointed trails of digital information. Valuable information is lost when goods change hands, cross borders and are transformed along the value chain. However, if these data can be harnessed and supply chains reconstituted, enormous opportunities exist to reduce friction, manage risk, enable financing and correct costly large-scale inefficiencies.</br>Helping businesses be part of something bigger</br>Clear provides a neutral and independent trade data network and AI platform. We work with B2B networks, logistics platforms, working capital providers and businesses to help them unlock more value from their trade data. We provide our partners with the opportunity to participate in a global trade network, enabling them to benefit from far-reaching network effects.</br>Greater Visibility</br>More transparency between buyers, sellers and consumers; greater visibility up and down supply chains; and a better way to Â manage risk.</br>Greater Efficiency</br>Independence, accountability and fairness are the foundations of Clear. We are also committed to the ethical application of AI and to enabling positive social and economic change. Our Principles ensure we stay true to these values. The activities of Clear are overseen by Clear Governance, an independent body of academics, technical specialists and our partners.</br>Clear will conduct its business in a transparent and accountable manner. We believe that the onus is on us to communicate and educate about the work that we do, and that it is our responsibility to foster a relationship of trust with stakeholders and the wider public.</br>Clear will ensure that our systems act in a fair and unbiased manner that actively encourages diversity, inclusion and accessibility. Clear is committed to processing personal data to a minimal extent, in full compliance with applicable law and our privacy policy</br>Clear will only use AI to enable positive social, economic and environmental change. We are committed to the responsible use of technology to enable transparency, efficiency and opportunity in markets everywhere.</br>We're hiring</br>If you're excited by the possibility of using \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to transform the global economy, and would be a good fit for one of the opportunities below, please get in touch. Weâre headquartered in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and offer challenging work, meaningful purpose, quality perks, competitive compensation, and excellent benefits.\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "webpages.ner_list[0].visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"companies\": [\n",
      "        \"AI\"\n",
      "    ],\n",
      "    \"people\": [],\n",
      "    \"keywords\": [\n",
      "        \"leave behind disjointed trails\",\n",
      "        \"something bigger clear provides\",\n",
      "        \"global economy ever made\",\n",
      "        \"independent trade data network\",\n",
      "        \"reaching network effects\",\n",
      "        \"actively encourages diversity\",\n",
      "        \"global trade network\",\n",
      "        \"processing personal data\"\n",
      "    ],\n",
      "    \"locations\": [\n",
      "        \"London\"\n",
      "    ],\n",
      "    \"events\": [],\n",
      "    \"products\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(webpages.ner_list[0].jsonify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AI': 3,\n",
       " 'London': 3,\n",
       " 'leave behind disjointed trails': 2,\n",
       " 'global economy ever made': 2,\n",
       " 'independent trade data network': 2,\n",
       " 'reaching network effects': 2,\n",
       " 'actively encourages diversity': 2,\n",
       " 'global trade network': 2,\n",
       " 'processing personal data': 2,\n",
       " 'something bigger clear provides': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salience = webpages.ner_list[0].get_salience()\n",
    "salience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extracted</th>\n",
       "      <th>Relative Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>London</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>leave behind disjointed trails</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>global economy ever made</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>independent trade data network</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>reaching network effects</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>actively encourages diversity</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>global trade network</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>processing personal data</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>something bigger clear provides</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Extracted Relative Importance\n",
       "0                               AI                1.00\n",
       "1                           London                1.00\n",
       "2   leave behind disjointed trails                0.67\n",
       "3         global economy ever made                0.67\n",
       "4   independent trade data network                0.67\n",
       "5         reaching network effects                0.67\n",
       "6    actively encourages diversity                0.67\n",
       "7             global trade network                0.67\n",
       "8         processing personal data                0.67\n",
       "9  something bigger clear provides                0.00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = [i for i in salience.values()]\n",
    "norm = [format(float(i)/max(vals),'1.2f') for i in vals]\n",
    "pd.DataFrame({'Extracted': [i for i in salience.keys()],'Relative Importance': norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n4j=Neo4j(webpages.ner_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Nodes added to graph:', 11)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n4j.add_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir(\"json\") == False:\n",
    "    !mkdir json\n",
    "\n",
    "for i in range(webpages.get_number_of_urls()):\n",
    "    json_text = webpages.ner_list[i].jsonify()\n",
    "    json_file = open(\"json/\" + webpages.ner_list[i].url,'w')\n",
    "    json_file.write(json_text)\n",
    "    json_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = [\"www.google.com\",\"www.balsejfnweo.cofm\"]\n",
    "np.savetxt(\"test_file.txt\", test_file, delimiter=\"\\n\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Everything(unittest.TestCase):\n",
    " \n",
    "    def test_url(self):\n",
    "        \n",
    "        obj = Extract(\"www.google.com\")\n",
    "        obj = Extract(\"test_file.txt\")\n",
    "        self.assertEqual(obj.get_number_of_urls(), 2)\n",
    "        self.assertEqual(str(obj.bad_urls[0]), \"http://www.balsejfnweo.cofm\")\n",
    "        \n",
    "    def test_ner_extraction(self):\n",
    "        \n",
    "        obj = Extract(\"test_file.txt\")\n",
    "        self.assertIsNone(obj.ner_list)\n",
    "        obj.extract_entities()\n",
    "        self.assertIsNotNone(obj.ner_list)\n",
    "        \n",
    "    def test_ner_class(self):\n",
    "        \n",
    "        obj = Extract(\"test_file.txt\")\n",
    "        obj.extract_entities()\n",
    "        \n",
    "        with self.assertRaises(IndexError):\n",
    "            obj.ner_list[2].url\n",
    "            \n",
    "        self.assertIsNotNone(obj.ner_list[0].jsonify())\n",
    "        self.assertIsNotNone(obj.ner_list[0].get_salience())\n",
    "    \n",
    "    def test_neo4j(self):\n",
    "        obj = Extract(\"test_file.txt\")\n",
    "        obj.extract_entities()\n",
    "        \n",
    "        self.assertIsNotNone(obj.ner_list)\n",
    "        for i in webpages.ner_list:\n",
    "            n4j=Neo4j(i)\n",
    "            n4j.add_nodes(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_neo4j (__main__.Test_Everything) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPConnectionPool(host='www.balsejfnweo.cofm', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f13b9b8aa20>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_ner_class (__main__.Test_Everything) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPConnectionPool(host='www.balsejfnweo.cofm', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f135d9355c0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_ner_extraction (__main__.Test_Everything) ... ok\n",
      "test_url (__main__.Test_Everything) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPConnectionPool(host='www.balsejfnweo.cofm', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f13b9ae29e8>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n",
      "HTTPConnectionPool(host='www.balsejfnweo.cofm', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f135e1fd198>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 4.021s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f13b0addcf8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
