{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /home/aa5118/anaconda3/envs/plural/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/aa5118/anaconda3/envs/plural/lib/python3.7/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/aa5118/anaconda3/envs/plural/lib/python3.7/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamedEntityRecognition:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        \n",
    "        self.url = url\n",
    "        self.companies = None\n",
    "        self.people = None\n",
    "        self.keywords = None\n",
    "        self.locations = None\n",
    "        self.events = None\n",
    "        self.products = None\n",
    "    \n",
    "    def jsonify(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extract:\n",
    "    \n",
    "    def __init__(self, filename):\n",
    "        \n",
    "        sources=pd.read_csv(filename,header=None,names=[\"url\"])\n",
    "        #sources=sources.append({\"url\":\"www.balsejfnweo.com\"}, ignore_index = True) \n",
    "        \n",
    "        self.sources = sources.head()\n",
    "        self.bad_urls = []\n",
    "        self.extracted = self.__extract_text()\n",
    "        self.ner_list = None\n",
    "        \n",
    "    def get_number_of_urls(self):\n",
    "        return len(self.sources)\n",
    "    \n",
    "    def __extract_text(self):\n",
    "        \n",
    "        extracted=[]\n",
    "        for url in self.sources['url']:\n",
    "\n",
    "            url = \"http://\"+url\n",
    "            \n",
    "            try:\n",
    "                html = requests.get(url)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.bad_urls.append(url)\n",
    "                print (e)\n",
    "                \n",
    "            if str(html) == \"<Response [200]>\":\n",
    "\n",
    "                soup = BeautifulSoup(html.text)\n",
    "\n",
    "                # kill all script and style elements\n",
    "                for script in soup([\"script\", \"style\"]):\n",
    "                    script.extract()    # rip it out\n",
    "\n",
    "                # get text\n",
    "                text = soup.get_text()\n",
    "\n",
    "                # break into lines and remove leading and trailing space on each\n",
    "                lines = (line.strip() for line in text.splitlines())\n",
    "                # break multi-headlines into a line each\n",
    "                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "                # drop blank lines\n",
    "                text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "            else:\n",
    "                text=\"\"\n",
    "\n",
    "            extracted.append(text)\n",
    "\n",
    "        return extracted\n",
    "    \n",
    "    def extract_entities(self):\n",
    "        \n",
    "        ner_list=[]\n",
    "        \n",
    "        for i, url in enumerate(self.sources['url']):\n",
    "            ner_list.append(NamedEntityRecognition(url))\n",
    "            \n",
    "            doc = nlp(extracted[i])\n",
    "            for entity in doc.ents:\n",
    "                if (entity.label_ == \"ORG\"):\n",
    "                    ner_list[0].companies = entity.text\n",
    "            \n",
    "        \n",
    "        self.ner_list = ner_list\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=Extract(\"./sources.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.bad_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'www.accrofab.co.uk'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.ner_list[0].url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
